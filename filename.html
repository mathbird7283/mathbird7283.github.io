<body style="background-color: #ffbbbb">
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML' async></script>
<script>
MathJax = {
tex: {
inlineMath: [['$', '$']]
}
};
</script>
<script id="MathJax-script" async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<h3><a href="project22-5.html"> << Page 4</a>  <a href="summary.html">Page 6 >> </a></h3>
<h2> Sensitivity Analysis </h2>

This project, it must be said, has its limits. 

<h3> <FONT COLOR = "blue">Assumptions and how we manage them </FONT></h3>

<p> There are not many assumptions we can break. One assumption I am making is that the 60-game season would extrapolate to a 162-game season in the way described previously. Perhaps one team would have gone on a 22-game winning streak (which, in the last six years of baseball, is not unprecedented), or the effects of losing players (or not) to COVID would have modified records. Without this assumption no analysis can be done.</p>

<p>Probably the biggest assumption I am making is the effect of <i>F</i> on <i>C</i>. My admittedly somewhat arbitrary criterion for <i>C</i> to equal 1 is if Randy is "At least 75% as much of a household name" as he would be otherwise. My best estimate is that if <i>F</i> were raised to 1 (indicating equivalent social interaction to 2019), then, all else being equal, <i>C</i> would have a 10 percent greater chance of equalling 1. The difference is fairly minimal due to the impact of the Internet.</p>

<p>Another assumption I am making is that <i>D</i> is linear in <i>A</i> and <i>B</i> (see below), and that when we take data points (e.g. television ratings over several years) they act independently. My method of comparing each year to the two adjacent ones circumvents the general decline in ratings.</p>


<h3> <FONT COLOR = "blue">Strategies and Alternatives</FONT></h3>

<p>First, we consider that there are no specifically defined unmeasured confounding variables -- at least, <i>none that are connected to A</i>. This is because the exact cause of the pandemic remains unknown, and regardless of what it is, a hypothetical U would not have affected other variables independently -- i.e. it is an instrument variable, connected only to <i>A</i>. This limits the types of sensitivity analyses that can viably be done.</p>

<p>Despite the lack of a defined U, we can do Manski sensitivity analysis on our given SWIG. There are two arrows that I consider to be likely to be monotone. Specifically, those are <i>B&rarr;F</i> and <i>E&rarr;G</i>. The former is monotone because restrictions caused by pandemics will most definitely limit how social people are. The latter is monotone because more postseason games will obviously mean more home runs.</p>

<h3> <FONT COLOR = "blue">What This Means Mathematically</FONT></h3>
<p> Consequently,

$$E[F(b)| A=a] \ge E[F(b')| A=a]$$

as long as b < b' and

$$E[G(e)| A=a, B=b] \ge E[G(e')| A=a, B=b]$$

as long as e>e'.

Thus, by the Manski result, 

$$E[F(2)-F(0)|A=a] \ge E[F|B=2,A=a]-E[F|B=0,A=a]$$ 

We know $$E[F|B=2,A=a]=0,$$ so the estimate of how much <i>F</i> changed can be bounded *below* by the expected <i>F</i> if <i>B</i>=0. Similarly, we can estimate the change in the value of G (home run count) by looking at the case in which the Rays don't make the postseason (corresponding to all but a select few values of E), but this is pointless as, in that case, $E[G]=0$.

</p>

<h3> <FONT COLOR = "blue">Expanding on the Last Page</FONT></h3>
<p> If we were to focus on an IPW/regression method to solve this problem, we would need to go through <i>D</i>. Considering the data set for TV ratings from 1989 to 2022, we run a simple linear regression on the data given, and we can estimate that

$$D = 0.5 + 0.1875A -0.125B.$$

This implies that if <i>B</i> changed from 2 to 0, there is a 75 percent chance that <i>D</i> would equal 0, and a 25 percent change that <i>D</i> would equal 1. Since <i>D</i> cannot be less than zero, this means that

$$0 \le E[D(a=1,b=0)] \le 0.25$$

by Manski bounding. One could also add an <i>AB</i> term (this breaks the assumption that the regression is purely linear). However, given the limited data, specifically that <i>B</i> is positive in only three years, render this somewhat pointless (however, if we had more data, it would certainly be worth doing!)
</p>

<p> Another way to interpret the data is to calculate a series of E-values. Suppose we disregard <i>A</i> and only intervene on <i>B</i>. Then <i>U,A,Y</i> in the examples become <i>A</i>, <i>B</i>, and <i>C</i> respectively. This means the E-value we seek is based on $RR_{BC}^{obs}$ -- and this determines whether <i>A</i> or <i>B</i> has a greater causal effect on <i>C</i>

<p>

The above relative risk is equal to $\frac{E[C|B=2]}{E[C|B=0]}$. We know the numerator is 1. What is $E[C|B=0]$? That is what we seek -- these allow us to interpret results rather than find them. It remains to be seen how exactly to find this expected value.

</p>


<h3> <FONT COLOR = "blue">Assumption Violation</FONT></h3>

<p>There are a few things that could happen if an assumption is violated. If we change our assumption that <i>A</i> is changeable and stop intervening on it, it becomes an (instrumental) unmeasured confounder. This allows for Rosenbaum or Vanderweele analysis. However, the classic sensitivity analysis has limited use in this example as we are not typically able to come up with the requisite data -- this is a hypothetical situation! Indeed, due to the limitations of my data, <i>there is no year</i> (since 1989, where my data begins) where <i>B</i> is nonzero and <i>D</i> is zero other than 2020, so the relative risk cannot be calculated for all of the arrows. Also, the value of $RR_{BA}$ does not really make sense in this context, as despite the arrow from <i>A</i> to <i>B</i>, the interactions between them are beyond the scope of this project. </p>

<p>There is also the possibility that there are relations in this SWIG that are not Markov compatible. However, if that is the case, we can rectify the problem by adding more arrows.</p>

<p>So, in general, we can use IPW and regression to give our best estimation of $E[C(a,b)]$, or more specifically $E[C(0,0)]$, assuming everything we've talked about actually holds.</p>

<h3> <FONT COLOR = "blue">Summary of What We Know + Future Investigation + A Pitfall?</FONT></h3>

<p> If <i>B=0</i> then <i>D</i> would rise to about 0.25. Meanwhile <i>G</i> is expected to be about 4.5, by the results on the previous page. <i>F</i>, by assumption, would increase to about 0.1. And from these and these alone we can, assuming Markov Compatibility, develope a system to estimate <i>C</i>. One potential pitfall of this is that a good measure of <i>C</i> is social media followers -- however, this severely limits our temporal historical data points to since about 2015, as prior to this year, benchmarks would be different.</p>

<p> For other examples of Manski bounding with instrumental confounders relating to daily life, consult Manski and Pepper's paper, "Monotone instrumental variables with an application to the returns to schooling." (1998) (available <a href="https://www.nber.org/system/files/working_papers/t0224/t0224.pdf">here</a>)

<a href="summary.html">summary</a>

</p>
</body>